# Complete OpenAI API Response Mock
# This mock represents a response with all possible OpenAI API fields
# Use model "mock-openai-complete" to access this response

responses:
  - text: "Hello! How can I assist you today?"
    model_used: "gpt-3.5-turbo-0125"
    prompt_tokens: 9
    completion_tokens: 12
    total_tokens: 21
    finish_reason: "stop"
    delay_ms: 100
    system_fingerprint: "fp_44709d6fcb"
    tool_calls:
      - id: "call_abc123"
        type: "function"
        function:
          name: "get_current_weather"
          arguments: "{\"location\": \"Boston, MA\"}"
    function_call:
      name: "get_current_weather"
      arguments: "{\"location\": \"Boston, MA\"}"
    logprobs:
      content:
        - token: "Hello"
          logprob: -0.31725305
          bytes: [72, 101, 108, 108, 111]
          top_logprobs:
            - token: "Hello"
              logprob: -0.31725305
              bytes: [72, 101, 108, 108, 111]
            - token: "Hi"
              logprob: -1.3190403
              bytes: [72, 105]

settings:
  mode: first

# This mock configuration generates a response equivalent to the complete OpenAI API response:
# {
#   "id": "chatcmpl-123abc",
#   "object": "chat.completion",
#   "created": 1677652288,
#   "model": "gpt-3.5-turbo-0125",
#   "system_fingerprint": "fp_44709d6fcb",
#   "choices": [
#     {
#       "index": 0,
#       "message": {
#         "role": "assistant",
#         "content": "Hello! How can I assist you today?",
#         "tool_calls": [
#           {
#             "id": "call_abc123",
#             "type": "function",
#             "function": {
#               "name": "get_current_weather",
#               "arguments": "{\"location\": \"Boston, MA\"}"
#             }
#           }
#         ],
#         "function_call": {
#           "name": "get_current_weather",
#           "arguments": "{\"location\": \"Boston, MA\"}"
#         }
#       },
#       "logprobs": {
#         "content": [
#           {
#             "token": "Hello",
#             "logprob": -0.31725305,
#             "bytes": [72, 101, 108, 108, 111],
#             "top_logprobs": [
#               {
#                 "token": "Hello",
#                 "logprob": -0.31725305,
#                 "bytes": [72, 101, 108, 108, 111]
#               },
#               {
#                 "token": "Hi",
#                 "logprob": -1.3190403,
#                 "bytes": [72, 105]
#               }
#             ]
#           }
#         ]
#       },
#       "finish_reason": "stop"
#     }
#   ],
#   "usage": {
#     "prompt_tokens": 9,
#     "completion_tokens": 12,
#     "total_tokens": 21
#   }
# }