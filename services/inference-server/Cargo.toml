[package]
name = "inference-server"
version = "0.1.0"
edition = "2024"

[dependencies]
axum = { version = "0.8.4", features = ["macros", "json"] }
tokio = { version = "1", features = ["full"] }
serde = { version = "1", features = ["derive"] }
serde_json = "1"
serde_yml = "0.0.12"
reqwest = { version = "0.12", features = ["json", "stream"] }
opentelemetry = { version = "0.30", features = ["logs"] }
opentelemetry_sdk = { version = "0.30", features = ["rt-tokio", "logs"] }
opentelemetry-stdout = { version = "0.30", features = ["logs"] }
opentelemetry-appender-tracing = "0.30"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter", "fmt", "json"] }
tracing-appender = "0.2"
config = "0.15"
uuid = { version = "1.2", features = ["v7"] }
rand = "0.9"
url = "2.5"
# Streaming support
futures-util = "0.3"
tokio-stream = { version = "0.1", features = ["sync"] }
eventsource-stream = "0.2"  # For parsing OpenAI's SSE responses
tower-http = { version = "0.6", features = ["limit", "timeout", "request-id", "util"] }

[dev-dependencies]
tempfile = "3"
